{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RyzVKoRpQoFX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data"
      ],
      "metadata": {
        "id": "vucs9OZ0d4Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
        "y_train = np.array([0, 0, 0, 1, 1, 1])"
      ],
      "metadata": {
        "id": "NXvKClkwd35-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sigmoid function**\n"
      ],
      "metadata": {
        "id": "bodhQUvgREOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Args:\n",
        "        z (ndarray): A scalar, numpy array of any size.\n",
        "\n",
        "    Returns:\n",
        "        g (ndarray): sigmoid(z), with the same shape as z\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    g = 1/(1+np.exp(-z))\n",
        "\n",
        "    return g"
      ],
      "metadata": {
        "id": "vK9GQc4uQ4Ip"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost Function**\n",
        "\n",
        "Recall that for logistic regression, the cost function is of the form\n",
        "\n",
        "𝐽(𝐰,𝑏)=1𝑚∑𝑖=0𝑚−1[𝑙𝑜𝑠𝑠(𝑓𝐰,𝑏(𝐱(𝑖)),𝑦(𝑖))](1)\n",
        "where\n",
        "\n",
        "𝑙𝑜𝑠𝑠(𝑓𝐰,𝑏(𝐱(𝑖)),𝑦(𝑖))\n",
        "  is the cost for a single data point, which is:\n",
        "\n",
        "𝑙𝑜𝑠𝑠(𝑓𝐰,𝑏(𝐱(𝑖)),𝑦(𝑖))=−𝑦(𝑖)log(𝑓𝐰,𝑏(𝐱(𝑖)))−(1−𝑦(𝑖))log(1−𝑓𝐰,𝑏(𝐱(𝑖)))(2)\n",
        "where m is the number of training examples in the data set and:\n",
        "𝑓𝐰,𝑏(𝐱(𝐢))𝑧(𝑖)𝑔(𝑧(𝑖))=𝑔(𝑧(𝑖))=𝐰⋅𝐱(𝑖)+𝑏=11+𝑒−𝑧(𝑖)(3)(4)(5)\n",
        "\n",
        "Code Description\n",
        "The algorithm for compute_cost_logistic loops over all the examples calculating the loss for each example and accumulating the total.\n",
        "\n",
        "Note that the variables X and y are not scalar values but matrices of shape (𝑚,𝑛\n",
        ") and (𝑚\n",
        ",) respectively, where 𝑛\n",
        " is the number of features and 𝑚\n",
        " is the number of training examples."
      ],
      "metadata": {
        "id": "kVsWJ1CjRJaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_logistic(X, y, w, b, *argv):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      y : (ndarray Shape (m,))  target value\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "    Returns:\n",
        "      total_cost : (scalar) cost\n",
        "    \"\"\"\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    loss_sum = 0\n",
        "\n",
        "    # Loop over each training example\n",
        "    for i in range(m):\n",
        "\n",
        "        # First calculate z_wb = w[0]*X[i][0]+...+w[n-1]*X[i][n-1]+b\n",
        "        z_wb = 0\n",
        "        # Loop over each feature\n",
        "        for j in range(n):\n",
        "            # Add the corresponding term to z_wb\n",
        "            z_wb_ij = w[j]*X[i][j]\n",
        "            z_wb += z_wb_ij\n",
        "        # Add the bias term to z_wb\n",
        "        z_wb += b # equivalent to z_wb = z_wb + b\n",
        "\n",
        "        f_wb = sigmoid(z_wb) # calculate prediction f_wb for a training example\n",
        "        loss =  -y[i] * np.log(f_wb) - (1 - y[i]) * np.log(1 - f_wb) # calculate loss for a training example\n",
        "\n",
        "        loss_sum += loss # equivalent to loss_sum = loss_sum + loss\n",
        "\n",
        "    total_cost = (1 / m) * loss_sum\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "PiOj8IwuQ4Fi"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculate the total cost of our example"
      ],
      "metadata": {
        "id": "jRrZqz4nebpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "initial_w = 0.01 * (np.random.rand(2) - 0.5)\n",
        "initial_b = -8\n",
        "\n",
        "print(\"total cost \",compute_cost_logistic(X_train, y_train, initial_w, initial_b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhe2_wHUeE6X",
        "outputId": "36f63168-24b8-46b2-c1e5-a95de1c66985"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total cost  3.9993297177889335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Descent Implementation**"
      ],
      "metadata": {
        "id": "wVtNXf2rWX9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_logistic(X, y, w, b, *argv):\n",
        "    \"\"\"\n",
        "    Computes the gradient for logistic regression\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "    Returns\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m,n = X.shape\n",
        "    dj_dw = np.zeros((n,))                           #(n,)\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
        "        err_i  = f_wb_i  - y[i]                       #scalar\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
        "        dj_db = dj_db + err_i\n",
        "    dj_dw = dj_dw/m                                   #(n,)\n",
        "    dj_db = dj_db/m                                   #scalar\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "mQ71EueUWXd3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the implementation of the gradient function using the cell below."
      ],
      "metadata": {
        "id": "FKW5aD6jepHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_train, y_train, w_tmp, b_tmp)\n",
        "print(f\"dj_db: {dj_db_tmp}\" )\n",
        "print(f\"dj_dw: {dj_dw_tmp.tolist()}\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcTYRGqPeqCJ",
        "outputId": "ce725b62-cc66-4f49-e030-867106dbb405"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dj_db: 0.0\n",
            "dj_dw: [-0.25, -0.16666666666666666]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient for logistic regression**"
      ],
      "metadata": {
        "id": "cyA6tJ-UWHO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X :    (ndarray Shape (m, n) data, m examples by n features\n",
        "      y :    (ndarray Shape (m,))  target value\n",
        "      w_in : (ndarray Shape (n,))  Initial values of parameters of the model\n",
        "      b_in : (scalar)              Initial value of parameter of the model\n",
        "      cost_function :              function to compute cost\n",
        "      gradient_function :          function to compute gradient\n",
        "      alpha : (float)              Learning rate\n",
        "      num_iters : (int)            number of iterations to run gradient descent\n",
        "      lambda_ : (scalar, float)    regularization constant\n",
        "\n",
        "    Returns:\n",
        "      w : (ndarray Shape (n,)) Updated values of parameters of the model after\n",
        "          running gradient descent\n",
        "      b : (scalar)                Updated value of parameter of the model after\n",
        "          running gradient descent\n",
        "    \"\"\"\n",
        "\n",
        "    # number of training examples\n",
        "    m = len(X)\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w_in = w_in - alpha * dj_dw\n",
        "        b_in = b_in - alpha * dj_db\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
        "            J_history.append(cost)\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
        "            w_history.append(w_in)\n",
        "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
        "\n",
        "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
      ],
      "metadata": {
        "id": "UMENJDLjQ38V"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data"
      ],
      "metadata": {
        "id": "5F7z0u-Hb51E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Some gradient descent settings\n",
        "iterations = 10000\n",
        "alpha = 0.001\n",
        "\n",
        "w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b,\n",
        "                                   compute_cost_logistic, compute_gradient_logistic, alpha, iterations, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXOQ2g39b5hr",
        "outputId": "53ecc21c-3c18-43b2-b2c8-0af69c044d38"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration    0: Cost     4.00   \n",
            "Iteration 1000: Cost     2.07   \n",
            "Iteration 2000: Cost     0.59   \n",
            "Iteration 3000: Cost     0.22   \n",
            "Iteration 4000: Cost     0.16   \n",
            "Iteration 5000: Cost     0.14   \n",
            "Iteration 6000: Cost     0.13   \n",
            "Iteration 7000: Cost     0.13   \n",
            "Iteration 8000: Cost     0.13   \n",
            "Iteration 9000: Cost     0.13   \n",
            "Iteration 9999: Cost     0.13   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost functions with regularization**"
      ],
      "metadata": {
        "id": "LGQFNUNMW3Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_logistic_regu(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      y : (ndarray Shape (m,))  target value\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "      lambda_ : (scalar, float) Controls amount of regularization\n",
        "    Returns:\n",
        "      total_cost : (scalar)     cost\n",
        "    \"\"\"\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    # Calls the compute_cost function that you implemented above\n",
        "    cost_without_reg = compute_cost_logistic(X, y, w, b)\n",
        "\n",
        "    # You need to calculate this value\n",
        "    reg_cost = 0.\n",
        "\n",
        "    for j in range(n):\n",
        "        reg_cost_j = w[j]**2 # calculate the cost from w[j]\n",
        "        reg_cost = reg_cost + reg_cost_j\n",
        "    reg_cost = (lambda_/(2 * m)) * reg_cost\n",
        "\n",
        "    # Add the regularization cost to get the total cost\n",
        "    total_cost = cost_without_reg + reg_cost\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "nK9YUGOnQ35H"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost = compute_cost_logistic_regu(X_train, y_train, initial_w, initial_b)\n",
        "\n",
        "print(\"Regularized cost :\", cost)"
      ],
      "metadata": {
        "id": "9Edh4yXAhOpJ",
        "outputId": "bd79ccd6-494f-4f6e-b063-4b1ce826b8dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regularized cost : 3.999330179690851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient descent with regularization**"
      ],
      "metadata": {
        "id": "H2tgqS6uXFzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_logistic_regu(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the gradient for logistic regression with regularization\n",
        "\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      y : (ndarray Shape (m,))  target value\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "      lambda_ : (scalar,float)  regularization constant\n",
        "    Returns\n",
        "      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b.\n",
        "      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w.\n",
        "\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "\n",
        "    dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    for j in range(n):\n",
        "\n",
        "        dj_dw_j_reg = (lambda_ / m) * w[j] # Your code here to calculate the regularization term for dj_dw[j]\n",
        "\n",
        "        # Add the regularization term  to the correspoding element of dj_dw\n",
        "        dj_dw[j] = dj_dw[j] + dj_dw_j_reg\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "RGqe2-FRXCHC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b,\n",
        "                                   compute_cost_logistic_regu, compute_gradient_logistic_regu, alpha, iterations, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfhbqaVZQ32E",
        "outputId": "47d9f9e6-be80-4601-d1ab-0100b6375330"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration    0: Cost     4.00   \n",
            "Iteration 1000: Cost     2.07   \n",
            "Iteration 2000: Cost     0.59   \n",
            "Iteration 3000: Cost     0.22   \n",
            "Iteration 4000: Cost     0.16   \n",
            "Iteration 5000: Cost     0.14   \n",
            "Iteration 6000: Cost     0.13   \n",
            "Iteration 7000: Cost     0.13   \n",
            "Iteration 8000: Cost     0.13   \n",
            "Iteration 9000: Cost     0.13   \n",
            "Iteration 9999: Cost     0.13   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating logistic regression"
      ],
      "metadata": {
        "id": "WlM57rS6gqL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w, b):\n",
        "    \"\"\"\n",
        "    Predict whether the label is 0 or 1 using learned logistic\n",
        "    regression parameters w\n",
        "\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "\n",
        "    Returns:\n",
        "      p : (ndarray (m,)) The predictions for X using a threshold at 0.5\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m, n = X.shape\n",
        "    p = np.zeros(m)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Loop over each example\n",
        "    for i in range(m):\n",
        "        z_wb = 0\n",
        "        # Loop over each feature\n",
        "        for j in range(n):\n",
        "            # Add the corresponding term to z_wb\n",
        "            z_wb_ij = X[i, j] * w[j]\n",
        "            z_wb += z_wb_ij\n",
        "\n",
        "        # Add bias term\n",
        "        z_wb += b\n",
        "\n",
        "        # Calculate the prediction for this example\n",
        "        f_wb = sigmoid(z_wb)\n",
        "\n",
        "        # Apply the threshold\n",
        "        p[i] = f_wb >= 0.5\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return p"
      ],
      "metadata": {
        "id": "SH4k_63Xb5er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute accuracy on our training set\n",
        "p = predict(X_train, w,b)\n",
        "print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkqBc9G4gyFP",
        "outputId": "634ec303-0020-4ed0-88f1-76e6a87985ac"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 100.000000\n"
          ]
        }
      ]
    }
  ]
}